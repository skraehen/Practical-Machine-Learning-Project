---
title: "Coursera - Practical Machine Learning Project"
author: "Stefan Krähenmann"
date: "Saturday, September 07, 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Background

Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These types of devices are part of the quantified self movement – a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. 


## The target of the project

In this project, the goal is to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the website here: http://web.archive.org/web/20161224072740/http:/groupware.les.inf.puc-rio.br/har (see the section on the Weight Lifting Exercise Dataset).


## Libraries
```{r }
library(caret)
library(parallel)
library(doParallel)
library(ggplot2)
library(RANN)
library(rattle)
library(corrplot)
```


## Loading the data from URL
```{r load}
training <- read.csv(url("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"),header=TRUE)
testdata <- read.csv(url("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"),header=TRUE)
```


# General overview of the dataset
```{r overview}
dim(training)
str(training)
```


## Data preparation

The training dataset comprises of 19622 observations on 160 columns. Many columns have NA values or blank values on almost every observation. 

Machine learning algorithms require complete datasets missing values have to be filled by estimating their values from the remaining data. However, this may introduce some erroneous values, particularly in case of a high amount of missing values. Also predictors containing low 
As a consequence, predictors containing a high fraction of missing values generally not add much useful information. Also predictors with low variability do not provide much insight. Therefore, columns with many missing values (more than 95 percent missing values) and with low variance will be discared. 

```{r preparation}
ind.na <- which(apply(is.na(training),c(2),mean) > .95) # check for missing data
length(ind.na) # number of predictors with high fraction of missing values
ind.nzv <- nearZeroVar(training) # check for data variance
length(ind.nzv) # number of predictors with low variability
```


The first seven columns give information about the participants of the test and timestamps. They will also not be taken into account.
```{r}
# Combine the indexes of  columns having at least 95% of missing values, low varoance and the first 7 columns
ind.remove <- unique(c(ind.na, ind.nzv, 1:7))
training <- training[,-ind.remove]
name <- names(training)
```


# Partition the training dataset into new training and validtion dataset
```{r partition}
set.seed(1201)
inTrain <- createDataPartition(training$classe, p=.7, list=F)
trainingPart <- training[inTrain,]
validationPart <- training[-inTrain,]
dim(trainingPart)
dim(validationPart)
```

This allows for an in sample validation of the machine learning algorithms. The new training dataset amounts to 70 percent of the original training dataset.


## Model buidling

For this project I used three different models, classification trees, random forests and generalized boosted models for prediction.
1.classification trees
2.random forests
3.Generalized Boosted Model


Cross-validation is used with 5 folds to limit the effects of overfitting and to improve the efficicency of the models. As relatievly complex machine learning algorithms are tested on a large dataset with many predictors, the processing time can be large. Therefor, calculations are done parallelly. All this is handelded by the parameter "trainingControl" which is then passed to caret's "train" function.

```{r control}
myControl <- trainControl(method = "cv",
                           number = 5,
                           allowParallel = TRUE,
                           verboseIter = FALSE)
# Allow for parallel calculations
cluster <- makeCluster(detectCores() - 3) # leave 3 core for OS
registerDoParallel(cluster)
```


## Prediction with classification trees
```{r tree}
set.seed(12)
model_tree <- train(classe~., trainingPart, method="rpart", trControl = myControl, tuneLength=3)
fancyRpartPlot(model_tree$finalModel)
```

```{r tree-predict}
trainpred.tree <- predict(model_tree,newdata=trainingPart)
confMat.tree <- confusionMatrix(trainingPart$classe,trainpred.tree)
# display confusion matrix and model accuracy
confMat.tree$table
train_acc.tree <- confMat.tree$overall[1]
train_acc.tree # show accuracy
```

The accuracy of this first model is very low (about 50%). This means that the outcome class will not be predicted very well using this model.


## Prediction with random forest
```{r rf}
set.seed(100)
model_rf <- train(classe~., trainingPart, method="rf", trControl = myControl, tuneLength=3, verbose=FALSE)
plot(model_rf,main="Accuracy of Random forest model by number of predictors")
```


```{r rf-predict}
trainpred.rf <- predict(model_rf,newdata=trainingPart)
confMat.rf <- confusionMatrix(trainingPart$classe,trainpred.rf)
# display confusion matrix and model accuracy
confMat.rf$table
train_acc.rf <- confMat.rf$overall[1] 
train_acc.rf # show accuracy
names(model_rf$finalModel) # show names of chosen predictors
# Compute the variable importance 
MostImpVars <- varImp(model_rf)
MostImpVars
```


Random forest reaches a much higher accuracy (> 99%) using 5-fold cross-validation than with classification tree. 


The optimal number of predictors (the number of predictors with the highest accuracy) is 27. There is no significal increase of the accuracy from 2 predictors to 27, but the accuracy slightly decreases with more than 27 predictors. "roll_belt" has shown to be the most important variable, it has been chosen in each cross-validation trial. 


## Prediction with gradient boosting
```{r gbm}
set.seed(35)
model_gbm <- train(classe~., trainingPart, method="gbm", trControl = myControl, tuneLength=3, verbose=FALSE)
plot(model_gbm)
```

```{r gbm-predict}
trainpred.gbm <- predict(model_gbm,newdata=trainingPart)
confMat.gbm <- confusionMatrix(trainingPart$classe,trainpred.gbm)
# display confusion matrix and model accuracy
confMat.gbm$table
train_acc.gbm <- confMat.gbm$overall[1]
train_acc.gbm # show accuracy
```

Also prediction with gradient boosting medthod is high (> 97 %). 


```{r validation}
# Predict using validation dataset
valpred.tree <- predict(model_tree,newdata=validationPart)
valpred.rf <- predict(model_rf,newdata=validationPart)
valpred.gbm <- predict(model_gbm,newdata=validationPart)

# accuracy with classification tree
val_acc.tree <- confusionMatrix(validationPart$classe,valpred.tree)$overall["Accuracy"]
val_acc.rf <- confusionMatrix(validationPart$classe,valpred.rf)$overall["Accuracy"]
val_acc.gbm <- confusionMatrix(validationPart$classe,valpred.gbm)$overall["Accuracy"]
```


# Compare in sample error with out of sample error
```{r compare}
df <- round(rbind(c(train_acc.tree, train_acc.rf, train_acc.gbm), c(val_acc.tree, val_acc.rf, val_acc.gbm)),2)
row.names(df) <- c("in-sample", "out-of-sample")
colnames(df) <- c("tree","rf","gbm")
df
```



## Conclusion

The out-of sample error is slightly higher than the in sample error. The lower in sampl error is because the models have been tuned with the training data, which always leads to some overfitting. Yet, the difference is very small here. The random forest model is overall the best one. Therefore, I will use the random forest model to predict the values of classe for the test data set.


## Application of the random forest model to the test dataset
```{r final}
prediction_final <- predict(model_rf, newdata=testdata[,-ind.remove])
prediction_final
```



```{r sequencial}
# Unregister cluster for parallel calculation
stopCluster(cluster)
registerDoSEQ()
```





